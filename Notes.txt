
## DATA PREPROCESSING
It's important to put different variables on a common scale, so that extreme variables 
don't have more weight than others
{x} scale - change the range of the values. The shape of the distribution doesn't change.
{x} normalize - bringing data points that were extreme before closer to the middle, 
so that they are less extreme (normal/Gaussian distribution). If you see a large standard deviation,
then you should probably normalize/standardize the data.

## IMPLEMENTATION (required by subject)
{x} prepocess unlabeled data
{x} split data into train set and validation set
{x} at least 2 hidden layers
{x} softmax activation function
{x} binary cross-entropy error function
{x} display at each epoch the training and validation metrics (model performance)
{x} implement a learning curve graph displayed at the end of the training phase
{x} training program
{x} prediction program

## BASIC STRUCTURE
{x} activation function
{x} Define the NN architecture
{x} initialise the parameters
{x} forward propagation
{x} back propagation
{x} cost

## BONUSES
{x} 3 different methods for scaling the data
{x} changeable batch size / minibatch vs whole batch
{x} multiple activation functions
{x} deep learning metrics
{x} two learning curves on one graph
{x} extensive data visualizations - accuracy, precision, recall, specificity, F1 Score
{x} display activations
{x} He weight initization

## TO DO
{ } incorporate N hidden layers
{x} restructure NN architecture
{x} change sigmoid -> ReLU (intermediary layers) and softmax (output layer)
{x} add loss functions
{x} batch size
{ } gradient checking
{ } optimization?
{ } regularization?
{ } confusion matrix
{x} accuracy, precision, recall, specificity, F1 Score
{ } PCA and Tsne? https://www.kaggle.com/djokester/visualising-the-breast-cancer-wisconsin-data-set

{x} make a prediction after __ epochs, then refine our parameters (gradient descent/learning)
{x} create function which calculates cost after each iteration
{x} add requirements.txt

TO FIX
{x} NN gives same output for every example / NN doesn't learn
{ } vanishing/exloding gradients w/ReLU
{ } weights go to NaN after _ iterations w/ReLU
{ } leaky_ReLU and prime - RuntimeWarning: invalid value encountered in greater_equal
{x} compute_loss - RuntimeWarning: divide by zero encountered in log
	^ I think due to log(0) -> nan
{x} softmax -  RuntimeWarning: invalid value encountered in subtract


DEBUG - vanishing/exploding gradients
{x} check X, y
{x} check X dtype - all float64
{x} check X for nans and nulls - none
{x} check self.y.shape, self.output.shape are same
{x} try y as one-hot-encoding vector
{x} check loss function correct
{x} check activation functions are correct
{x} numerical stability for loss function - added epsilon
{x} numerical stability for softmax - shift the highest value in array to zero.
{x} try different activation functions
{x} try different number of neurons
{x} try different learning rate
{x} try different number of iterations
{x} try a dataset of just 2, then 5 examples
{x} add He weight initization (for ReLU) – * np.sqrt(2/N_in), where N_in = input_neurons (or fan_in, neurons from previous layer)
{ } add Xavier weight initization (for Tanh) – * np.sqrt(2 / (N_in + N_out))	 source: keras https://github.com/keras-team/keras/blob/998efc04eefa0c14057c1fa87cab71df5b24bf7e/keras/initializations.py
{x} add Lecun weight initization (for efficient backprop?) – * np.sqrt(3/N_in)
{ } weight or L1/L2 regularization
{ } gradient checking
{ } gradient clipping
{ } momentum-based SGD
{ } batch normalization
{ } threshhold hyperparameter

OBSERVATIONS
- At 0.01 learning rate and 20000 iterations, and whole batch...
a NN of this architecture:
- weights are features x neurons 
- biases are 1 x neurons
- forwardprop is calculated input * weights
- backprop is calculated d_w = activation * d_z
... performs at 60-90% accuracy
while a NN of this architecture:
- weights are neurons x features
- biases are neurons x 1
- forwardprop is calculated weights * input
- backprop is calculated d_w = d_z * activation
... performs at 98.45% accuracy

- possible overfitting with 0.1% learning rate?

