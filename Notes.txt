
## DATA PREPROCESSING
It's important to put different variables on a common scale, so that extreme variables 
don't have more weight than others
{x} scale - change the range of the values. The shape of the distribution doesn't change.
{x} normalize - bringing data points that were extreme before closer to the middle, 
so that they are less extreme (normal/Gaussian distribution). If you see a large standard deviation,
then you should probably normalize/standardize the data.


## IMPLEMENTATION (required by subject)
{x} split data into train set and validation set
{x} at least 2 hidden layers
{} softmax activation function
{} display at each epoch the training and validation metrics (model performance)
{x} implement a learning curve graph displayed at the end of the training phase
{} training program
{} prediction program - binary cross-entropy error function

## BASIC STRUCTURE
{x} activation function
{x} Define the NN architecture
{x} initialise the parameters
{x} forward propagation
{x} back propagation
{x} cost

## TO DO
{} incorporate N hidden layers
{} restructure NN architecture
{} change sigmoid -> ReLU (intermediary layers) and softmax (output layer)
{} add loss functions
{} gradient checking
{} optimization?
{} regularization?
{} batch size



{} make a prediction after __ epochs, then refine our parameters (gradient descent/learning)
{} create function which calculates cost after each iteration

TO FIX
{} output is always almost the same / NN gives same output for every example
{} vanishing/exloding gradients
{} weights go to NaN after _ iterations
{} NN doesn't learn
{x} compute_loss - RuntimeWarning: divide by zero encountered in log
    ^ I think due to log(0) -> nan
{} softmax -  RuntimeWarning: invalid value encountered in subtract
{} leaky_ReLU and prime - RuntimeWarning: invalid value encountered in greater_equal


DEBUG - vanishing/exploding gradients
{x} check X, y
{x} check self.y.shape, self.output.shape are same
{x} check loss function correct
{x} check activation functions are correct
{x} try different activation functions
{x} try different number of neurons
{x} try different learning rate
{x} try different number of iterations
{x} try a dataset of just 2, then 5 examples
{x} add He weight initization (for ReLU) – * np.sqrt(2/N_in), where N_in = input_neurons (from previous layer)
{} add Xavier weight initization (for Tanh) – * np.sqrt(1/N_in)
{} add Xavier/Bengio weight initization – * np.sqrt(2/N_avg), where N_avg = (N_in+N_out/2)