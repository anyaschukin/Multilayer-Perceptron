
## DATA PREPROCESSING
It's important to put different variables on a common scale, so that extreme variables 
don't have more weight than others
{} scale - change the range of the values. The shape of the distribution doesn't change.
{} normalize - bringing data points that were extreme before closer to the middle, 
so that they are less extreme (normal/Gaussian distribution). If you see a large standard deviation,
then you should probably normalize/standardize the data.


## IMPLEMENTATION (required by subject)
{} split data into train set and validation set
{x} at least 2 hidden layers
{} softmax activation function
{} display at each epoch the training and validation metrics (model performance)
{x} implement a learning curve graph displayed at the end of the training phase
{} training program
{} prediction program - binary cross-entropy error function

## BASIC STRUCTURE
{x} activation function
{x} Define the NN architecture
{x} initialise the parameters
{x} forward propagation
{x} back propagation
{x} cost

## TO DO
{} incorporate N hidden layers
{} restructure NN architecture
{} change sigmoid -> ReLU (intermediary layers) and softmax (output layer)
{} add loss functions
{} gradient checking
{} optimization?
{} regularization?
{} batch size



{} make a prediction after __ epochs, then refine our parameters (gradient descent/learning)
{} create function which calculates cost after each iteration